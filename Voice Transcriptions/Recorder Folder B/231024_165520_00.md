Listening to episode 27 of Awakening from a Meaning Crisis again at 6 minutes and 35
seconds, he starts talking about... it's the beginning of him talking about commentarial
explosion, it's where he first lays it out, and he references Keith Holio as the person
who used the example of doing in Simon and Neal's problem space or search space formalisation
on a chess game and how that ends up being a ridiculous amount of choices.
Apparently this Keith Holio was a psychologist who was instrumental in doing psychological
work on the psychology of problem solving.
I really want to go through the commentarial explosion stuff and relevance realisation
stuff with the Philosophers. So basically the very end of episode 26 where they lay
out problem formalisation, like the Simon and Neal's problem space or search space,
and then the first like maybe 15 minutes or so of episode 27 where he lays out the actual
problem of how big commentarial explosion is. He lays out commentarial explosion and
then lays out the fact that we don't search it, we don't check it, we just somehow intelligently
ignore most of it and focus on only what's relevant to us.
Note, if you do do that and go through it with the boys, he talks about the 9 dot problem
at about 13 minutes and most people aren't... well, they'll probably kind of remember it
but Ryan probably might not have seen it before and Nick and Jaden probably at least... Nick
probably won't remember it. I don't know, I'm not sure. But Nick and Jaden, it's worth
going over again so you should probably show a little bit of the 9 dot problem and then
just be like, keep this in your mind. He brings it up later just so we can understand what
it is. It's not directly connected to what he's going to talk about straight away, it's
just so we understand the example he gives later on.
He also then mentions after that that he's argued in multiple papers that the problem,
this basically the combinatorial explosion and relevant realization problem is what the
search for AGI is trying to solve at the moment and I think that's his... what is it?
The problem that the project of artificial intelligence is trying to address right now.
In fact, that's what I argued. I've argued him some work I did with Tim Lillicrap and
Blake Richards, some work I've done with Leo Ferraro.
Tim Lillicrap, Blake Richards and Leo Ferraro.
Worked by other people.
Then he goes on to talk about how Stanovic mentions that Stanovic argues... basically
a few people pointing out Stanovic have argued that the best way for us to define intelligence
is actually that it's our ability to basically do relevant realization. It's our ability
to avoid cognitive limitations, avoid combinatorial explosion by intelligently focusing on things.
Zeroing in on stuff.
And Christopher Czerniak argued something similar apparently.
Also, this is directly related to wanting to... this is actually related to what I've
just been talking about is stuff that he's referencing that I want to read up, which
is that the difference, the distinction between heuristics and algorithms was coined by Paul
Yer in a book called How to Solve It.
He talks about how algorithms work in terms of certainty and the problem with that is
that to get certainty we would have to search the entire problem space and the problem space's
algorithm is combinatorially explosive.
My question is, and I think the question that anybody watching this, like any smart person
watching this like Ryan is probably going to ask this question, "Is it?" is to get
certainty do you actually... do you have to search the entire search space, problem space?
My sense is that yes, because otherwise any... there's always, even if it's an infinitesimally
small chance, there's always a chance that the... one of the problems that you didn't
check somehow is wrong.
Or falls outside or doesn't meet the criteria you're looking for.
And honestly I actually think searching the entire space still isn't necessarily going
to work.
I definitely do get that sense but I am biased towards believing what John Verbecky says
and I think this is a point, this is something that somebody would probably, I can imagine
Ryan or Jaden probably disagreeing with this.
Very much imagine Jaden disagreeing with this.
Just out of no algorithms work, what are you talking about?
So that's an interesting thing to ponder and I don't really have an answer other than I
feel like yeah probably you do have to search, like I feel like, I feel quite strongly actually
that yes you probably do have to search the entire search space to get certainty and actually
I think even then you probably don't get certainty because you could have searched it wrong.
But anyway, I think Ryan will probably have a problem with what he says at 18 minutes
as well that like, where he's talking about we can't be comprehensively logical, we're
not just logical because you can't always be logical because deductive logic, he equates
this with deductive logic which I'm not sure is actually right, but logic because I'm pretty
sure inductive is also logic, but deductive logic works in terms of certainty and therefore
we can't actually do that.
It's interesting because we do sometimes use deductive logic, but what's happening there?
Are we, I mean I guess it's you are assuming, you are shrinking the search space by assuming
that if A, I can't remember how they fucking say it, it's the if Socrates is a man, all
men are mortal, Socrates is man therefore Socrates is mortal, which I think is in logical terms
is A, if A, it's A and then B and then AB equals C basically, based on, yeah, based
on that, but you are assuming that A and B are true and you're not certain that they're
true, you are just assuming, you are making the assumption, you are choosing that as a
starting point from which to work which is necessarily shrinking the search space, but
they may not be true.
I mean we can be pretty sure that Socrates is a man and that all men are mortal, but
then we're stuck with the problem of induction then, maybe there is a man out there who is
immortal, and we just haven't found him yet, and you have to use induction to get those
points, you use induction to get your A and B, you have to use induction to get your A
and B and then based on your A and B you're getting your deduction, but you still have
to go through the process, the combinatorially explosive process of getting A and B through
induction.
That's actually a really interesting problem as well, like that's actually a really interesting,
how does the problem of induction and combinatorial explosion, how does on one side the problem
of induction and on the other side combinatorial explosion and relevance realisation, how do
those two go together?
And also how does induction and deduction go together and how do we get from induction
to deduction and in what instances do we use induction and in what instances do we use
deduction?
If there's anybody out there who's talked about how deduction does this, in terms of
to deduce something you have to be assuming that if A is true and B is true, C therefore
must be true, it's certain that C is true, but you are assumed, but A and B have to be
true and to do this deduction you have to assume that A and B are true, basically, and
they might not be, which means to do that deduction you have to shrink the problem space.
This also really raises a really interesting problem of how it is if everything we deal
with in life is combinatorial.
I think it seems like pretty much everything we deal with in life is combinatorially explosive.
If that's the case, how is it that maths and deductive logic are not like that, that they
can work in terms of certainty if the rest of everyday life doesn't work like that?
Because those two things, deductive logic and mathematics, have been the basis for a
lot of science and philosophy in the way we've tried to deal with things because of their
certainty, because of the fact they work with certainty, but if that doesn't actually work
in reality in our everyday lived lives, how do they work?
I suspect my initial suspicion is that they work by the small worlds.
Small worlds, not small worlds, like the tiny worlds or small worlds or whatever they're
called, small world simulations, whether they are not simulating the whole of reality, they're
simulating a small piece of it.
In a deduction, you are already automatically...
The small world, a deduction, a deductive logic, is already a small world simulation
because you are assuming that A and B are the case.
You're already shrinking the model down from all of reality to a reality where A and B
are the case, where A and B are true, in order to assume that C is true, in order to deduce
that C is true.
So it makes a bit of sense with deductive logic, but how does it work with maths?
This is an important point.
At 21 minutes and 50-ish seconds, he says that what heuristics are doing is that they
are getting you to pay attention to certain parts of the problem space, and to do that,
they are...
This leads into the thing that they're intimately tied up with bias.
They are getting you to prejudge...
A heuristic is getting you to prejudge what will be relevant to you.
They are getting you to prejudge...
A heuristic is making you...
In using a heuristic, you are prejudging, therefore prejudice, you are prejudging what
is relevant in the situation, in the thing, which is why it leads to bias, and why we,
therefore, cannot be anything but biased, because we have to be biased to be able to
deal with the complexity, to deal with the combinatorially explosive complexity of reality,
which is...
This is the most important point.
This means that starting with...
To explain my views, I have to start with combinatorial explosion.
Basically lay out exactly...
Just do exactly what John Gavreki's argument is here.
Start with that and then lay it.
From there, go into...
Well, actually not from there.
Start with that and then maybe see if you can expand it or say it in a different way
so you're not just copying exactly what he says, but basically work through what he talks
about at the end of...
I don't think he needs to go up like the intelligence and all that kind of stuff, but
just the fact that reality is infinitely complex, you start off with combinatorial explosion
and explaining it through the way he does, and then from there, link to algorithms and
heuristics, and then from there, link to the fact that heuristics are...
Well, actually, that's a step in between, because that's the step from that to we'll
always be biased, which means that we should be understanding of each other.
Which is an important step, but that's a step beyond reality is infinitely complex.
So just start with combinatorial explosion and then work up through there and then go
back and add extra points or the other points around...
Basically, from the last five to 10 minutes of episode 26 to up to around 21 minutes or
so into episode 27, listen to that stuff, and from there, you will get how to start
with the beginning of explaining your views around why reality is infinitely complex,
or infinitely is the wrong term, because it might not be infinitely complex.
It may just be very, very, very, very largely complex.
And is different from very big.
So infinite's the wrong word.
It could be infinite, but it's more like it's so large that it might as well be infinite
for our purposes.
Though it may not actually be.
There may actually be a set amount.
It may not be an infinite amount.
It may be an actual number.
So start from there, talk about combinatorial explosion, work from there out to how that
leads to us having to be biased to deal with the world and the ethical considerations that
come into that, and then from there, work backwards, go backwards and add in extra evidence
and extra reasons and extra stuff to draw on as to why you think the world's infinitely
complex.
It should probably take a while, actually.
I think I will probably get a lot of pushback from people who like the less wrong crowd,
like honestly probably Jaden and Ryan, the crowd that wants to believe that we can have
certainty in our beliefs.
That wants to, that thinks that, because he's basically saying we have to use heuristics
in our, like the whole basis, the centre of my argument for, centre of my argument for
the infinite complexity of the world is that, centre of my argument for the infinite complexity
of the world is that, or wrapped up in it is that we have to use, well actually no,
it's really more of an outcome, but we have to use, because of combinatorial explosion,
we have to use heuristics.
We cannot be algorithmic in our thinking.
We are not comprehensively logical.
We don't work in terms of certainty.
And I think there's a large crowd of people, all the rationalists and the less wrong people
and the, you know, the, lots of people.
And actually I think this is maybe why I'm realising I have so many issues and get so
frustrated with the rationalist crowd, because of, you know, and effective altruists and
people like this.
The Bill dude I was listening to, I was listening to the Firebug, yeah, Aussie Firebug podcast
today and just Bill's something he's talking about in his book, Live to Die with Zero or
Die at Zero or something where you, and he's talking about optimising, just like optimising
and breaking down every part of your life, basically, and it's just, I think, it's because
we can't, that doesn't embrace ambiguity.
That's rejecting ambiguity and rejecting the complexity of the world.
And there's like a desperate need for there to be a possibility of certainty, you know,
and a desperate need for us to be logical creatures because there desperately needs
that certainty.
And I think that maybe is what motivates a lot of the people who are like, you know,
talk about using Bayesian inference to update their beliefs and stuff like that.
I mean that's probabilistic, that's not certainty.
But it still, it kind of strikes us, smacks us of the same thing in some ways.
It's not exactly the same.
I mean it is accepting, I guess, that there is uncertainty because you're working in terms
of probability rather than certainty.
But that crowd that wants to systematise and rationalise and formalise everything, like
the less wrong, effective altruism, Berkeley rationalists, all that kind of stuff.
And I think in some ways, Jayden and particularly I actually think Ryan are heavy proponents,
I think Ryan's a heavy proponent of leaning into towards being a heavy proponent of that
side of things.
The way he's talked about things leans towards that to me.
And that I think will, I will get pushback on this, you know, we need to use heuristics
thing.
Yeah.
Maybe I can just start off with writing a really short, basically laying out Jayden,
John Vivecki's argument around heuristics and how that leads to us.
Commitment to explosion leads to us being, which I'm inferring that the world is infinitely
complex or highly, highly complex or uncountably complex based on committment to explosion
and maybe one or two other factors.
From that it follows that we have to use heuristics, we can't use logic, we can't use deductive
logic and algorithms in our day to day lives.
We don't work that way.
And therefore, because we have to use heuristics to function, we are always going to, you know,
and yeah, evidence for that is all of these fucking heuristics that, you know, actually
that perfectly works with the fact that, you know, so much of the evidence that has been
coming out of behavioural economics and psychology for the last 30 years since Kahneman has been
about all of our irrational, like the ways in which we are not actually rational.
Which pushes back against the economic orthodoxy.
That all ties together.
Fucking perfect.
Fucking perfect.
